{
 "cells": [
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PARSE NEWS DATA**"
   ]
  },
  {
>>>>>>> task3
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "# Import Data\n",
    "def load_datasets(paths):\n",
    "    \"\"\"Load datasets from specified paths and display the first few rows.\"\"\"\n",
    "    dfs = [pd.read_csv(path) for path in paths]\n",
    "    for df in dfs:\n",
    "        print(df.head())\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Column Names and Shape\n",
    "def print_info(df):\n",
    "    \"\"\"Print column names and the shape of the DataFrame.\"\"\"\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Domain\n",
    "def extract_domain(df, url_column='url'):\n",
    "    \"\"\"Extract the domain from URLs in the specified column.\"\"\"\n",
    "    df['domain'] = df[url_column].apply(lambda x: tldextract.extract(x).domain)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Mapping\n",
    "def map_sentiment_scores(df, sentiment_column='sentiment'):\n",
    "    \"\"\"Map sentiment values to numerical scores.\"\"\"\n",
    "    sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "    df['sentiment_score'] = df[sentiment_column].map(sentiment_mapping)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "def merge_dataframes(dfs, on_keys):\n",
    "    \"\"\"Merge multiple DataFrames on common keys.\"\"\"\n",
    "    merged_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=on_keys, how='left')\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Missing Values\n",
    "def check_missing_values(df):\n",
    "    \"\"\"Check and report missing values in each column.\"\"\"\n",
    "    return df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "def handle_missing_values(df, fill_values):\n",
    "    \"\"\"Handle missing values by filling with predefined values.\"\"\"\n",
    "    df.fillna(value=fill_values, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics and Data Types\n",
    "def print_summary_statistics(df):\n",
    "    \"\"\"Print summary statistics and data types of the DataFrame.\"\"\"\n",
    "    print(df.describe(include='all'))\n",
    "    print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Irrelevant Columns and Feature Engineering\n",
    "def drop_irrelevant_columns(df, columns_to_drop):\n",
    "    \"\"\"Drop irrelevant columns from the DataFrame.\"\"\"\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Content Length and Title Word Count\n",
    "def calculate_content_length(df, content_column='content'):\n",
    "    \"\"\"Calculate the length of the content in the DataFrame.\"\"\"\n",
    "    df['content_length'] = df[content_column].apply(len)\n",
    "    return df\n",
    "\n",
    "def calculate_title_word_count(df, title_column='title'):\n",
    "    \"\"\"Calculate the word count of the title in the DataFrame.\"\"\"\n",
    "    df['title_word_count'] = df[title_column].apply(lambda x: len(x.split()))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Boxplots for Numerical Columns\n",
    "def plot_boxplots(df, numerical_columns):\n",
    "    \"\"\"Plot boxplots for numerical columns to detect outliers.\"\"\"\n",
    "    for column in numerical_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=df[column])\n",
    "        plt.title(f'Boxplot for {column}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Outliers Using Z-Score\n",
    "from scipy import stats\n",
    "\n",
    "def find_outliers_zscore(df, numerical_columns, threshold=3):\n",
    "    \"\"\"Identify outliers based on Z-score.\"\"\"\n",
    "    outliers = {}\n",
    "    for column in numerical_columns:\n",
    "        z_scores = stats.zscore(df[column].dropna())\n",
    "        outliers[column] = df[(abs(z_scores) > threshold)].index.tolist()\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms\n",
    "def plot_histograms(df, numerical_columns):\n",
    "    \"\"\"Plot histograms for numerical columns.\"\"\"\n",
    "    for column in numerical_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df[column].dropna(), kde=True)\n",
    "        plt.title(f'Histogram for {column}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Bar Charts for Categorical Data\n",
    "def plot_bar_charts(df, categorical_columns):\n",
    "    \"\"\"Plot bar charts for categorical data.\"\"\"\n",
    "    for column in categorical_columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x=df[column])\n",
    "        plt.title(f'Bar Chart for {column}')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Keywords Using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords_tfidf(df, text_column='text'):\n",
    "    \"\"\"Extract top keywords using TF-IDF.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(df[text_column])\n",
    "    scores = X.sum(axis=0).A1\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "    return dict(zip(keywords, scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize Region\n",
    "def categorize_region(df, location_column='location'):\n",
    "    \"\"\"Categorize websites based on location.\"\"\"\n",
    "    # Define predefined regions and mapping logic here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Visitors\n",
    "def estimate_visitors(df, rank_column='global_rank'):\n",
    "    \"\"\"Estimate website visitors based on global rank.\"\"\"\n",
    "    # Define logic for estimation\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Keywords with Error Handling\n",
    "def extract_keywords_safe(df, text_column='text'):\n",
    "    \"\"\"Safely extract keywords from text with error handling.\"\"\"\n",
    "    try:\n",
    "        return extract_keywords_tfidf(df, text_column)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extracting keywords: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Similarity\n",
    "def calculate_similarity(keywords1, keywords2):\n",
    "    \"\"\"Calculate similarity between two sets of keywords using intersection and union.\"\"\"\n",
    "    set1 = set(keywords1)\n",
    "    set2 = set(keywords2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(df, text_column='text'):\n",
    "    \"\"\"Calculate cosine similarity between text documents using TF-IDF.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(df[text_column])\n",
    "    return cosine_similarity(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by tokenizing, lemmatizing, and removing stopwords.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def train_naive_bayes_classifier(X_train, y_train):\n",
    "    \"\"\"Train a Naive Bayes classifier on TF-IDF vectorized text data.\"\"\"\n",
    "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the trained model using accuracy score and classification report.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Model with MLflow\n",
    "import mlflow\n",
    "\n",
    "def log_model_with_mlflow(model, X_test, y_test):\n",
    "    \"\"\"Log the trained model to MLflow with parameters and performance metrics.\"\"\"\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params({'model': 'Naive Bayes'})\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "        y_pred = model.predict(X_test)\n",
    "        mlflow.log_metric('accuracy', accuracy_score(y_test, y_pred))\n",
    "        mlflow.log_text(classification_report(y_test, y_pred), 'classification_report.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PostgreSQL Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Design and Creation\n",
    "def create_postgres_schema():\n",
    "    \"\"\"Create a PostgreSQL schema for storing ML features.\"\"\"\n",
    "    # Define schema creation logic here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def load_data_to_postgres(df, table_name, conn_str):\n",
    "    \"\"\"Load the processed DataFrame into PostgreSQL.\"\"\"\n",
    "    engine = create_engine(conn_str)\n",
    "    df.to_sql(table_name, engine, if_exists='"
   ]
>>>>>>> task3
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
